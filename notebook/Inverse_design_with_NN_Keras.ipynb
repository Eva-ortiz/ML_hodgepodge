{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse design with Keras Neural Networks\n",
    "\n",
    "### Level: Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to achieve the inverse desing of the input parameters to provide to an ANN in order to achieve an specific output. \n",
    "\n",
    "First, we create a basic neural network with optimized hiperparameters [1,2]. Then, we adress the optimization process between the desired output and the ANN output as a function of the ANN inputs.\n",
    "\n",
    "References and additional documentation at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds\n",
    "seed = 0\n",
    "np_rng = np.random.default_rng(seed)  # [3]\n",
    "keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Load train data and split into train/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = pd.read_parquet(\"features.parquet\")\n",
    "    y = pd.read_parquet(\"targets.parquet\")\n",
    "except FileNotFoundError:\n",
    "    # For didactic purposes, we define a set of data with binary representation\n",
    "    # of 0 to 3 as input and the decimal representation as output.\n",
    "    X = pd.DataFrame(\n",
    "        [\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 0],\n",
    "            [0, 1],\n",
    "            [0, 1],\n",
    "            [1, 1],\n",
    "            [1, 0],\n",
    "            [1, 1],\n",
    "            [0, 0],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        * 4\n",
    "    )\n",
    "    y = pd.Series([0, 1, 2, 2, 1, 1, 3, 2, 3, 0, 0] * 4)\n",
    "\n",
    "\n",
    "# we set a fixed random state for reproducibility and teaching purposes,\n",
    "# but our results have to be consistent across multiple seeds to be relevant\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.9, test_size=0.1, random_state=seed\n",
    ")\n",
    "\n",
    "# shape of input features and output predictions\n",
    "features_shape = X_train.iloc[0].shape\n",
    "target_shape = y_train.iloc[0].shape if bool(y_train.iloc[0].shape) else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model optimizer\n",
    "loss = \"mae\"\n",
    "lr = {\"min_value\": 1e-4, \"max_value\": 1e-2}  # values from 0.0001 to 0.01\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "# training\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "# hp tuner\n",
    "max_trials = 50\n",
    "\n",
    "# early stopping\n",
    "monitor = \"val_loss\"\n",
    "patience = int(0.1 * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model\n",
    "\n",
    "Hyperparameter optimization and NN model training.\n",
    "\n",
    "Optimization regarding epoch number is skipped as early stopping is considered a sufficient method for achieving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\"Build a neural network model.\"\"\"\n",
    "    input_layer = keras.Input(shape=features_shape)\n",
    "    inner_layer_1 = keras.layers.Dense(64, activation=\"selu\")(input_layer)\n",
    "    inner_layer_2 = keras.layers.Dense(32, activation=\"selu\")(inner_layer_1)\n",
    "    inner_layer_3 = keras.layers.Dense(16, activation=\"selu\")(inner_layer_2)\n",
    "    output_layer = keras.layers.Dense(target_shape)(inner_layer_3)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"NN_model\")\n",
    "\n",
    "    # Tune the learning rate for the optimizer, choose an optimal value [2]\n",
    "    hp_learning_rate = hp.Float(\n",
    "        \"learning_rate\", min_value=lr[\"min_value\"], max_value=lr[\"max_value\"]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Nadam(learning_rate=hp_learning_rate),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set a fixed random state for reproducibility and teaching purposes\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel=model_builder,\n",
    "    objective=metrics,\n",
    "    max_trials=max_trials,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    seed=seed,\n",
    "    project_name=\"KerasTuner\",\n",
    "    # executions_per_trial = 10\n",
    ")\n",
    "\n",
    "# set an early stopping\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=monitor, patience=patience)]\n",
    "\n",
    "# finally tune hyperpararmeters using the 10 percent of train data for\n",
    "# validation\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# train the model again\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# save the model\n",
    "model.save(\"../models/model__Inverse_design_with_NN_Keras.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, load a saved model\n",
    "\n",
    "Load saved model and use it for the inverse design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.saving.load_model(\"../models/model__Inverse_design_with_NN_Keras.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse design\n",
    "\n",
    "First, define the loss which determine the difference between our target and the output of the network as a function of the inputs of the network, i.e., define the objective function to be minimized. \n",
    "\n",
    "We select the Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(NN_input: np.array, model: keras.Model, target: float) -> float:\n",
    "    \"\"\"Obtain the Mean Squared Error between an ANN output and a given target.\n",
    "\n",
    "    Intended for a single NN input, not a batch of inputs.\n",
    "    \"\"\"\n",
    "    if len(NN_input.shape) > 2:\n",
    "        raise ValueError(\"Function intended just for a single NN input. Terminated.\")\n",
    "\n",
    "    # add an aditional dimension to `NN_input` to add the batch size dimension,\n",
    "    # which is just one\n",
    "    NN_input_batched = np.expand_dims(NN_input, axis=0)\n",
    "\n",
    "    # obtain the NN output using the model\n",
    "    NN_output = model(NN_input_batched)\n",
    "    return (target - NN_output) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, specify the boundaries of each input (due to selected solver [4]) and store several radom initializations for the initial guess of  best inputs, so we assure the global minimum can be reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_initializations = 5\n",
    "int_features_shape = features_shape[0]  # indexing specific of this example\n",
    "\n",
    "# set wide bounds to stress test our application\n",
    "# common bound for all inputs of the ANN\n",
    "lower_bound = 0\n",
    "higher_bound = 1\n",
    "input_bounds = [[lower_bound, higher_bound]] * int_features_shape\n",
    "\n",
    "# random initializations of input parameters\n",
    "init_guesses = np_rng.uniform(\n",
    "    low=lower_bound, high=higher_bound, size=(n_initializations, int_features_shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select `3-point` as the method to compute the gradient method [4,5].\n",
    "\n",
    "We finally select the target to retrieve and employ scipy minimization to reach the true optimal input for each initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the target as the desired output decimal number for this example\n",
    "# we select something to test the retrieval of the closest input\n",
    "target = 2\n",
    "expected_opt = [1, 0]\n",
    "\n",
    "retrieved_inputs = []\n",
    "for init_guess in init_guesses:\n",
    "    # In order to tune the `minimize` function, `method` is an interesting\n",
    "    # input, which can require to specify `bounds` input with a sequence of\n",
    "    # (min, max) pairs for each element in x\n",
    "    solver_output = minimize(\n",
    "        mse_loss,\n",
    "        init_guess,\n",
    "        args=(model, target),\n",
    "        jac=\"3-point\",\n",
    "    )\n",
    "    print(f\"\\nSolver optimization sucsess: {solver_output.success}\")\n",
    "    if not solver_output.success:\n",
    "        print(f\"Cause of termination: {solver_output.message}\")\n",
    "\n",
    "    # analyze obtained optimized inputs\n",
    "    print(\n",
    "        f\"\\nGuess:{init_guess} -> {np.round(init_guess, 0)}\",\n",
    "        f\"\\nExpected: {expected_opt}\"\n",
    "        f\"\\nObtained: {solver_output.x} -> {np.round(solver_output.x, 0)}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    ".. [1] In notebook folder, [`Training_NN_Keras.ipynb`](https://github.com/Eva-ortiz/ML_hodgepodge/blob/main/notebook/Training_NN_Keras.ipynb)\n",
    "\n",
    ".. [2] In notebook folder, [`Hp_optimization_NN_Keras.ipynb`](https://github.com/Eva-ortiz/ML_hodgepodge/blob/main/notebook/Training_NN_Keras.ipynb)\n",
    "\n",
    ".. [3] https://numpy.org/doc/stable/reference/random/index.html \n",
    "\n",
    ".. [4] https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html \n",
    "\n",
    ".. [5] https://www.vaia.com/en-us/textbooks/math/numerical-analysis-9-edition/chapter-4/problem-6-use-the-most-accurate-three-point-formula-to-deter/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
