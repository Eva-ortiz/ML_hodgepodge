{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the hyperparameters of a Neural Network with Keras\n",
    "\n",
    "### Level: Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to use Keras tuner to optimize ANN hiperparameters. \n",
    "\n",
    "First, we create a basic neural network for this task. Then, we adress how to optimize some inner and outer hiperparameters.\n",
    "\n",
    "At the end of this notebook there is some useful documentation of this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 19:18:09.338029: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-20 19:18:09.338773: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-20 19:18:09.343914: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-20 19:18:09.359397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732126689.382905   68955 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732126689.391007   68955 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 19:18:09.420127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Load train data and split into train/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = pd.read_parquet(\"features.parquet\")\n",
    "    y = pd.read_parquet(\"targets.parquet\")\n",
    "except FileNotFoundError:\n",
    "    X = pd.DataFrame(\n",
    "        [\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 0],\n",
    "            [0, 1],\n",
    "            [0, 1],\n",
    "            [1, 1],\n",
    "            [1, 0],\n",
    "            [1, 1],\n",
    "            [0, 0],\n",
    "            [0, 0],\n",
    "        ]\n",
    "    )\n",
    "    y = pd.Series([0, 1, 2, 2, 1, 1, 3, 2, 3, 0, 0])\n",
    "\n",
    "\n",
    "# we set a fixed random state for reproducibility and teaching purposes,\n",
    "# but our results have to be consistent across multiple seeds to be relevant\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.9, test_size=0.1, random_state=0\n",
    ")\n",
    "\n",
    "# also fix keras random seed\n",
    "keras.utils.set_random_seed(0)\n",
    "\n",
    "# shape of input features and output predictions\n",
    "features_shape = X_train.iloc[0].shape\n",
    "target_shape = y_train.iloc[0].shape if bool(y_train.iloc[0].shape) else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model optimizer\n",
    "loss = keras.losses.MeanAbsoluteError()  # or \"mae\"\n",
    "lr = {\"min_value\": 1e-4, \"max_value\": 1e-2}  # values from 0.0001 to 0.01\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "# training\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "# hp tuner\n",
    "max_trials = 50\n",
    "\n",
    "# early stopping\n",
    "monitor = \"val_loss\"\n",
    "patience = int(0.1 * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network model\n",
    "\n",
    "We employ the `Functional API` inside of a function to define a model for hyperparameter search [1], with an input for hyperparameters to search as indicated.\n",
    "\n",
    "In this example, we just go for the optimization of the learning rate, this is, how much we change the NN parameters each train step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\"Build a neural network model.\"\"\"\n",
    "    input_layer = keras.Input(shape=features_shape)\n",
    "    inner_layer_1 = keras.layers.Dense(64, activation=\"selu\")(input_layer)\n",
    "    inner_layer_2 = keras.layers.Dense(32, activation=\"selu\")(inner_layer_1)\n",
    "    inner_layer_3 = keras.layers.Dense(16, activation=\"selu\")(inner_layer_2)\n",
    "    output_layer = keras.layers.Dense(target_shape)(inner_layer_3)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"NN_model\")\n",
    "\n",
    "    # Tune the learning rate for the optimizer, choose an optimal value [2]\n",
    "    hp_learning_rate = hp.Float(\n",
    "        \"learning_rate\", min_value=lr[\"min_value\"], max_value=lr[\"max_value\"]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Nadam(learning_rate=hp_learning_rate),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Hyperparameter optimization\n",
    "\n",
    "Defined the model, select the method to achieve the tuning [3] and perform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ./KerasTuner/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 1\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'linear'}\n",
      "\n",
      "The hyperparameter search is complete. The optimal learning \n",
      "    rate for the optimizer is 0.008515000000000002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 19:18:12.385752: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# we set a fixed random state for reproducibility and teaching purposes, but our\n",
    "# results have to be consistent across multiple seeds to be relevant, which can\n",
    "# be easily implemented avoiding the fixing of a `seed` and setting\n",
    "# `executions_per_trial` to more than one\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel=model_builder,  # model to tune that takes hyperparameters and returns a Model instance\n",
    "    objective=metrics,  # direction of the optimization\n",
    "    max_trials=max_trials,  # total number of model configurations to test\n",
    "    tune_new_entries=True,  # if hyperparameter entries requested by the hypermodel should be added to the search space\n",
    "    allow_new_entries=True,\n",
    "    seed=0,\n",
    "    project_name=\"KerasTuner\",  # prefix for files saved by this Tuner, which are control points for each model configuration\n",
    "    # executions_per_trial = 10\n",
    ")\n",
    "# NOTE: if hyperparameter search is executed again, Keras Tuner will use\n",
    "# `project_name` saved files to resume the search. To avoid that, set\n",
    "# `overwrite=True`.\n",
    "\n",
    "# set an early stopping for training\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=monitor, patience=patience)]\n",
    "\n",
    "# finally tune the hyperpararmeters, using the 10 percent of train data for\n",
    "# validation. Input arguments are the same than those for keras.model.fit [4]\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# print search space summary\n",
    "tuner.search_space_summary(extended=False)\n",
    "\n",
    "# get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\n",
    "    f\"\"\"\\nThe hyperparameter search is complete. The optimal learning \n",
    "    rate for the optimizer is {best_hps.get('learning_rate')}.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Hyperparameter optimization\n",
    "\n",
    "Found the optimal inner hyperparameters, use them for training and find the optimal outer hyperparameters, such as the number of epochs during training.\n",
    "\n",
    "This epoch optimization can be safely skipped if, for example, early stopping is considered to be enough to avoid the lost of performance or the saving of the models at different epochs is implemented. [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 1.4377 - val_accuracy: 1.0000 - val_loss: 0.1571\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3750 - loss: 0.3712 - val_accuracy: 1.0000 - val_loss: 0.6839\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.3750 - loss: 0.2796 - val_accuracy: 1.0000 - val_loss: 0.7806\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3750 - loss: 0.7899 - val_accuracy: 1.0000 - val_loss: 0.0344\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.3750 - loss: 0.0744 - val_accuracy: 1.0000 - val_loss: 0.0587\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.3750 - loss: 0.5230 - val_accuracy: 0.0000e+00 - val_loss: 0.5108\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 0.2529 - val_accuracy: 1.0000 - val_loss: 0.0677\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3750 - loss: 0.0881 - val_accuracy: 1.0000 - val_loss: 0.1015\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.3750 - loss: 0.3819 - val_accuracy: 1.0000 - val_loss: 0.3212\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.3750 - loss: 0.1593 - val_accuracy: 1.0000 - val_loss: 0.1567\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.3750 - loss: 0.1596 - val_accuracy: 1.0000 - val_loss: 0.1797\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3750 - loss: 0.2510 - val_accuracy: 1.0000 - val_loss: 0.1861\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.3750 - loss: 0.1927 - val_accuracy: 1.0000 - val_loss: 0.1294\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.3750 - loss: 0.1856 - val_accuracy: 1.0000 - val_loss: 0.1883\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3750 - loss: 0.1974 - val_accuracy: 1.0000 - val_loss: 0.0697\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.3750 - loss: 0.1300 - val_accuracy: 1.0000 - val_loss: 0.1517\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3750 - loss: 0.1627 - val_accuracy: 1.0000 - val_loss: 0.0835\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.3750 - loss: 0.1314 - val_accuracy: 1.0000 - val_loss: 0.1552\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3750 - loss: 0.1730 - val_accuracy: 1.0000 - val_loss: 0.0125\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.3750 - loss: 0.0584 - val_accuracy: 1.0000 - val_loss: 0.0794\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3750 - loss: 0.0283 - val_accuracy: 1.0000 - val_loss: 0.2529\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.3750 - loss: 0.1338 - val_accuracy: 1.0000 - val_loss: 0.0746\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.3750 - loss: 0.0362 - val_accuracy: 1.0000 - val_loss: 0.1248\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.3750 - loss: 0.2516 - val_accuracy: 1.0000 - val_loss: 0.0322\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.3750 - loss: 0.0297 - val_accuracy: 1.0000 - val_loss: 0.0379\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.3750 - loss: 0.1754 - val_accuracy: 1.0000 - val_loss: 0.0413\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.3750 - loss: 0.0328 - val_accuracy: 1.0000 - val_loss: 0.0924\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.3750 - loss: 0.0550 - val_accuracy: 1.0000 - val_loss: 0.0714\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3750 - loss: 0.1574 - val_accuracy: 1.0000 - val_loss: 0.1462\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.3750 - loss: 0.0667 - val_accuracy: 1.0000 - val_loss: 0.0177\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.3750 - loss: 0.1473 - val_accuracy: 1.0000 - val_loss: 0.1472\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.3750 - loss: 0.0567 - val_accuracy: 1.0000 - val_loss: 0.1229\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.3750 - loss: 0.1019 - val_accuracy: 1.0000 - val_loss: 0.0884\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.3750 - loss: 0.0841 - val_accuracy: 1.0000 - val_loss: 0.0890\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.3750 - loss: 0.1365 - val_accuracy: 1.0000 - val_loss: 0.0582\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.3750 - loss: 0.0427 - val_accuracy: 1.0000 - val_loss: 0.1081\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3750 - loss: 0.1572 - val_accuracy: 1.0000 - val_loss: 0.0270\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.3750 - loss: 0.0354 - val_accuracy: 1.0000 - val_loss: 0.2529\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.3750 - loss: 0.0797 - val_accuracy: 1.0000 - val_loss: 0.0204\n",
      "\n",
      "Best epoch: 19\n"
     ]
    }
   ],
   "source": [
    "# build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# train the model again\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# obtain the epoch with the best loss\n",
    "val_loss_per_epoch = history.history[monitor]\n",
    "best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "print(\"\\nBest epoch: %d\" % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last train with optimized hyperparameters\n",
    "\n",
    "Finally, train the final version of the model with inner and outer optimized hyperparameters, evaluate and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 1.5043 - val_accuracy: 1.0000 - val_loss: 0.2221\n",
      "Epoch 2/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3750 - loss: 0.4666 - val_accuracy: 1.0000 - val_loss: 0.3304\n",
      "Epoch 3/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3750 - loss: 0.5005 - val_accuracy: 0.0000e+00 - val_loss: 0.6251\n",
      "Epoch 4/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 0.4581 - val_accuracy: 1.0000 - val_loss: 0.0041\n",
      "Epoch 5/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.3750 - loss: 0.1805 - val_accuracy: 1.0000 - val_loss: 0.7392\n",
      "Epoch 6/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.3750 - loss: 0.1895 - val_accuracy: 1.0000 - val_loss: 0.2253\n",
      "Epoch 7/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3750 - loss: 0.3849 - val_accuracy: 1.0000 - val_loss: 0.2934\n",
      "Epoch 8/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.3750 - loss: 0.0832 - val_accuracy: 1.0000 - val_loss: 0.2248\n",
      "Epoch 9/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3750 - loss: 0.1092 - val_accuracy: 1.0000 - val_loss: 0.2645\n",
      "Epoch 10/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3750 - loss: 0.3833 - val_accuracy: 1.0000 - val_loss: 0.4039\n",
      "Epoch 11/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.3750 - loss: 0.1936 - val_accuracy: 1.0000 - val_loss: 0.1751\n",
      "Epoch 12/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3750 - loss: 0.3173 - val_accuracy: 1.0000 - val_loss: 0.2457\n",
      "Epoch 13/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3750 - loss: 0.0789 - val_accuracy: 1.0000 - val_loss: 0.2476\n",
      "Epoch 14/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.3750 - loss: 0.3506 - val_accuracy: 1.0000 - val_loss: 0.2414\n",
      "Epoch 15/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.3750 - loss: 0.0621 - val_accuracy: 1.0000 - val_loss: 0.1678\n",
      "Epoch 16/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.3750 - loss: 0.3020 - val_accuracy: 1.0000 - val_loss: 0.2414\n",
      "Epoch 17/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3750 - loss: 0.0624 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
      "Epoch 18/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3750 - loss: 0.2079 - val_accuracy: 1.0000 - val_loss: 0.1575\n",
      "Epoch 19/19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.3750 - loss: 0.0809 - val_accuracy: 1.0000 - val_loss: 0.2847\n",
      "\n",
      " test ['loss', 'compile_metrics'] : [0.15606850385665894, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# build again the model with the optimal hyperparameters\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# retrain the model for the last time\n",
    "hypermodel.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=best_epoch,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "score = hypermodel.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n test {hypermodel.metrics_names} : {score}\")\n",
    "\n",
    "model = hypermodel.save(\"../models/model__Hp_optimization_NN_Keras.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    ".. [1] https://www.tensorflow.org/tutorials/keras/keras_tuner?hl=es-419 \n",
    "\n",
    ".. [2] https://keras.io/api/keras_tuner/hyperparameters/\n",
    "\n",
    ".. [3] https://keras.io/api/keras_tuner/tuners/\n",
    "\n",
    ".. [4] https://keras.io/api/keras_tuner/tuners/base_tuner/ \n",
    "\n",
    ".. [5] https://keras.io/api/callbacks/model_checkpoint/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
