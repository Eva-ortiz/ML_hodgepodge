{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the hyperparameters of a Neural Network with Keras\n",
    "\n",
    "### Level: Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to use Keras tuner to optimize ANN hiperparameters. \n",
    "\n",
    "First, we create a basic neural network for this task. Then, we adress how to optimize some inner and outer hiperparameters.\n",
    "\n",
    "At the end of this notebook there is some useful documentation of this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "\n",
    "import keras_tuner as kt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Load train data and split into trai/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet(\"features.parquet\")\n",
    "y = pd.read_parquet(\"targets.parquet\")\n",
    "\n",
    "# we set a fixed random state for reproducibility and teaching purposes,\n",
    "# but our results have to be consistent across multiple seeds to be relevant\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, train_size=0.9, test_size=0.1, random_state=0\n",
    ")\n",
    "\n",
    "# shape of input features and output predictions\n",
    "features_shape = X_train.iloc[0].shape\n",
    "target_shape = y_train.iloc[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model optimizer\n",
    "loss = keras.losses.MSE()\n",
    "lr = {\"min_value\": 1e-4, \"max_value\": 1e-2}  # values from 0.0001 to 0.01\n",
    "metrics = \"val_loss\"\n",
    "\n",
    "# training\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "early_stopping_patience = int(0.1 * epochs)\n",
    "\n",
    "# hp tuner\n",
    "max_trials = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network model\n",
    "\n",
    "We employ the `Functional API` inside of a function to define a model for hyperparameter search [1], with an input for hyperparameters to search as indicated.\n",
    "\n",
    "In this example, we just go for the optimization of the learning rate, this is, how much we change the NN parameters each train step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "\n",
    "    input_layer = keras.Input(shape=features_shape)\n",
    "    inner_layer_1 = keras.layers.Dense(64, activation=\"selu\")(input_layer)\n",
    "    inner_layer_2 = keras.layers.Dense(32, activation=\"selu\")(inner_layer_1)\n",
    "    inner_layer_3 = keras.layers.Dense(16, activation=\"selu\")(inner_layer_2)\n",
    "    output_layer = keras.layers.Dense(target_shape)(inner_layer_3)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"NN_model\")\n",
    "\n",
    "    # Tune the learning rate for the optimizer, choose an optimal value [2]\n",
    "    hp_learning_rate = hp.Float(\n",
    "        \"learning_rate\", min_value=lr[\"min_value\"], max_value=lr[\"max_value\"]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Nadam(learning_rate=hp_learning_rate),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Hyperparameter optimization\n",
    "\n",
    "Defined the model, select the method to achieve the tuning [3] and perform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set a fixed random state for reproducibility and teaching purposes, but our\n",
    "# results have to be consistent across multiple seeds to be relevant, which can\n",
    "# be easily implemented avoiding the fixing of a `seed` and setting\n",
    "# `executions_per_trial` to more than one\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel=model_builder,  # model to tune that takes hyperparameters and returns a Model instance\n",
    "    objective=metrics,  # direction of the optimization\n",
    "    max_trials=max_trials,  # total number of model configurations to test\n",
    "    tune_new_entries=True,  # if hyperparameter entries requested by the hypermodel should be added to the search space\n",
    "    allow_new_entries=True,\n",
    "    seed=0,\n",
    "    project_name=\"KerasTuner\",  # prefix for files saved by this Tuner, which are control points for each model configuration\n",
    "    # executions_per_trial = 10\n",
    ")\n",
    "# NOTE: if hyperparameter search is executed again, Keras Tuner will use\n",
    "# `project_name` saved files to resume the search. To avoid that, set\n",
    "# `overwrite=True`.\n",
    "\n",
    "# set an early stopping for training\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=metrics, patience=early_stopping_patience)\n",
    "]\n",
    "\n",
    "# finally tune the hyperpararmeters, using the 10 percent of train data for\n",
    "# validation. Input arguments are the same than those for keras.model.fit [4]\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# print search space summary.\n",
    "tuner.search_space_summary(extended=False)\n",
    "\n",
    "# get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\n",
    "    f\"\"\"\n",
    "The hyperparameter search is complete. The optimal learning rate for the\n",
    "optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Hyperparameter optimization\n",
    "\n",
    "Found the optimal inner hyperparameters, use them for training and find the optimal outer hyperparameters, such as the number of epochs during training.\n",
    "\n",
    "This epoch optimization can be safely skipped if, for example, early stopping is considered to be enough to avoid the lost of performance or the saving of the models at different epochs is implemented. [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# train the model again\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=metrics, patience=early_stopping_patience)\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# obtain the epoch with the best loss\n",
    "val_loss_per_epoch = history.history[metrics]\n",
    "best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "print(\"Best epoch: %d\" % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last train with optimized hyperparameters\n",
    "\n",
    "Finally, train the final version of the model with optimized hyperparameters, evaluate and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build again the model with the optimal hyperparameters\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# retrain the model for the last time\n",
    "hypermodel.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=best_epoch,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "score = hypermodel.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"[test loss, test accuracy]:\", score)\n",
    "\n",
    "model = keras.saving.load_model(\"opt_hp_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    ".. [1] https://www.tensorflow.org/tutorials/keras/keras_tuner?hl=es-419 \n",
    "\n",
    ".. [2] https://keras.io/api/keras_tuner/hyperparameters/\n",
    "\n",
    ".. [3] https://keras.io/api/keras_tuner/tuners/\n",
    "\n",
    ".. [4] https://keras.io/api/keras_tuner/tuners/base_tuner/ \n",
    "\n",
    ".. [5] https://keras.io/api/callbacks/model_checkpoint/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
