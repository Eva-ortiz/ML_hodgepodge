{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of a Neural Network with Keras\n",
    "\n",
    "### Level: Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use Keras library to create a basic neural network. Then, we adress the pipeline to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies\n",
    "\n",
    "First, we have to install from terminal the libraries we are going to use:\n",
    "\n",
    "- pip install pandas\n",
    "- pip install scikit-learn\n",
    "- pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**\n",
    "\n",
    "For a cleaner version, we create what is called an `environment`, which we will use to install all needed packages [2].\n",
    "\n",
    "In `VSCode`, just go to `Terminal`>`New Terminal` and:\n",
    "\n",
    "- Install conda, changing the second line with the version for your OS [1].\n",
    "```\n",
    "mkdir -p ~/miniconda3\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "rm -rf ~/miniconda3/miniconda.sh\n",
    "```\n",
    "- Create and activate your environment.\n",
    "```\n",
    "conda create -n NN_training\n",
    "conda activate NN_training\n",
    "```\n",
    "\n",
    "- Install your libraries as previously indicated.\n",
    "\n",
    "- Activate the environment in your notebook, if needed. In VSCode, click on `Select Kernel`>`Python Environments...`>`NN_training`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies\n",
    "\n",
    "Once installed, we import in our program the necessary functions of those dependencies to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "This will be different for each proyect. Once you have enough and valuable data (which we all know is a difficult task!), it has to be loaded and processed depending of their format, even normalizing/standardizing it, which is a highly recommended practice [7]. \n",
    "\n",
    "A personal recomendation is to use `pandas` for this task.\n",
    "\n",
    "Once we have meaningful and clean data, we split it into `train`/`test` splits, being the first the data used for training our Neural Network and the last to evaluate its performance [3]. \n",
    "\n",
    "A common split is 80/20, so we use it as starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our clean and nice normalized data is saved in features/targets parquets\n",
    "X = pd.read_parquet(\"features.parquet\")\n",
    "y = pd.read_parquet(\"targets.parquet\")\n",
    "\n",
    "# we set a fixed random state for reproducibility and teaching purposes,\n",
    "# but our results have to be consistent across multiple seeds to be relevant\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network definition\n",
    "\n",
    "Although we can define Multi-layer Perceptrons with scikit-learn library [4], we adress the use of Keras for higher flexibility.\n",
    "\n",
    "With Keras, we can set the model layers several ways, here we adress what they call the `Functional API` [5, 6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of input features and output predictions\n",
    "features_shape = X_train.iloc[0].shape\n",
    "target_shape = y_train.iloc[0].shape\n",
    "\n",
    "# input layer, which receives X_train\n",
    "input_layer = keras.Input(shape=features_shape)\n",
    "# inner layers, with SELU activation function as recommended in [7]\n",
    "inner_layer_1 = keras.layers.Dense(64, activation=\"selu\")(input_layer)\n",
    "inner_layer_2 = keras.layers.Dense(32, activation=\"selu\")(inner_layer_1)\n",
    "# output layer, no activation as we are not simulating a classification model\n",
    "output_layer = keras.layers.Dense(target_shape)(inner_layer_2)\n",
    "\n",
    "# determined the layers, create the model\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"NN_model\")\n",
    "\n",
    "# specify some others parameters left, again with recomendations in [7]\n",
    "model.compile(\n",
    "    loss=keras.losses.MSE(),\n",
    "    optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        \"val_loss\"\n",
    "    ],  # TODO: probar para confirmar la nomenclatura: val_loss, val_mse o val_mae\n",
    ")\n",
    "\n",
    "# see our created model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, some regularization techniques like dropout layers can be added, but we skip it for future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparameter optimization and training\n",
    "\n",
    "[WIP] Once we have our model, we have to find the hiperparameters to use, such as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References \n",
    ".. [1] https://docs.anaconda.com/miniconda/ \n",
    "\n",
    ".. [2] https://code.visualstudio.com/docs/python/environments#_work-with-python-interpreters \n",
    "\n",
    ".. [3] https://scikit-learn.org/1.5/modules/cross_validation.html#cross-validation \n",
    "\n",
    ".. [4] https://scikit-learn.org/1.5/modules/neural_networks_supervised.html\n",
    "\n",
    ".. [5] https://keras.io/getting_started/intro_to_keras_for_engineers/ \n",
    "\n",
    ".. [6] https://keras.io/guides/functional_api/\n",
    "\n",
    ".. [7] A. Géron. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media, 2019.\n",
    "\n",
    ".. [8] https://keras.io/about/  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
